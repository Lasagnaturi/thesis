\chapter{Related works}

When training a Reinforcement Learning agent several problems arise, especially when the learning process moves from simulated environments to the real world. In this section, a few useful techniques for the purposes of this thesis, are presented.

\section{State Representation Learning} \label{sec:srl}
% checked
Reinforcement Learning is a very general method for learning sequential decision-making tasks. On the other hand, Deep Learning has become in recent years the best set of algorithms capable of Representation Learning (ReL), i.e. a class of algorithms that are designed to extract abstract features from data. A mix of the two provides a particularly powerful framework for learning state representation, especially when dealing with real-world environments that tend to be much more complex and unpredictable than simulated environments. In particular, State Representation Learning (SRL) is a specific type of ReL where extracted features are in low dimension, evolve over time and are affected by the agent actions. The low dimensionality allows easier interpretation by humans, it mitigates the curse of dimensionality and it speeds up the policy learning process. Thus, SRL is well suited for Deep Reinforcement Learning applications. \citet{DBLP:journals/corr/abs-1802-04181} presented a complete survey that covers the state-of-the-art on SRL. Feature extraction is a set of algorithms whose objective, as the name suggests, is to decompose a particular data point into smaller identifiable components that are useful for the learning task at hand. For example, in a dataset composed of images collected by the DonkeyCar, in the context of autonomous driving, a set of features that can compose each picture can be the curve's direction and the curve's angle. Training a neural network to learn those features may be accomplished by compressing the image into a smaller vector, discarding all the unnecessary information that is not relevant for the learning task, where each dimension would represent a feature like the ones just described. However, a feature not necessarily describes a human interpretable aspect of the data, rather it can even lack semantic meaning. In particular, SRL techniques exploit the time steps, actions and eventually rewards, to transform observations into representative low dimensional vectors that contain the most relevant features to learn a particular policy. The better the policy or the speed with which it is learned, the more the features extracted are significant to the model. In particular, the authors described and compared methods that do not require explicit supervision such as AEs, VAEs, Principal Component Analysis~\citep{https://doi.org/10.48550/arxiv.1505.00322} and Position-Velocity Encoders~\citep{DBLP:journals/corr/JonschkowskiHSR17}.

In our work we use AEs/VAEs to decouple state representation learning from policy learning, in order to speed up the training process.

\section{Improving sample efficiency} \label{sec:sampleefficiency}
% checked
The observation space of the agent in our experiments is an image, as perceived by the camera of the DonkeyCar, both simulated and real. However, training a model from high-dimensional images with RL is ineffective as \citet{DBLP:journals/corr/abs-1910-01741} show. In their work the authors propose to use SRL to mitigate such issue. 

%Deep convolutional encoders can learn a good representation even though they generally require large amounts of training data. Using off-policy methods and adding an auxiliary task, i.e. an additional cost-function that an RL agent can predict and observe from the environment, with an unsupervised objective can naturally improve sample efficiency and add stability in optimization. In particular, \citet{DBLP:journals/corr/abs-1910-01741} propose a simple and effective autoencoder-based off-policy method that can be trained end-to-end. Their main focus is on finding the optimal way of training an RL agent using SRL.

In particular, they propose a simple and effective autoencoder-based off-policy method that can be trained end-to-end. Their main focus is on finding the optimal way of training an RL agent using SRL. The authors, in their experiments, selected an AutoEncoder (AE) as an SRL technique which is composed of a convolutional encoder that maps an image observation to a low dimensional latent space and a deconvolutional decoder that reconstructs the latent vector back to the original image. The objective is to minimize the image reconstruction loss avoiding task-dependent losses. Then the SAC algorithm is used to learn different tasks directly from the latent space learned by the encoder, thus speeding up the training.

In practice, they experimented with two ways of training the agent. In the first one the AE is pre-trained offline with the states/observations collected from the environment. Then, when training the agent online, both the encoder and the agent are updated but independently of each other. In other words, the agent is trained with transitions coming from the replay buffer, while the AE only uses the states in the buffer to update its latent representation. Moreover, the authors investigated different update frequencies of the AE.

In the second training strategy, the setting is equivalent to the first one except that the AE is trained according to the task the agent is solving. The idea is that the representations that the AE learns should be related to the RL objective such that the learned features can be useful for the task at hand. However, the results obtained by the authors show that updating the AE with the agent gradients hurts the performance of the agent, since the AE is shared between the actor and the critic.

Furthermore, the results also show that updating the AE at the end of each episode and independently from the agent, results in the best performance in a wide range of tasks. On the other hand, using a pre-trained AE without updating it when training the agent shows a comparable performance and we use such a strategy to train the agent in our experiments. Indeed, updating the AE at the end of each episode would require an additional computation that would increase the training time when learning the policy in the real world.

\section{Smooth exploration}
% checked
When moving an RL algorithm from a simulated environment to the real world, the unstructured step-based exploration, i.e. without taking into consideration the underlying hardware, often leads to unstable motion patterns that may result in poor exploration, longer training time, and even damage to the robot the agent is controlling. \citet{pmlr-v164-raffin22a} handles such issue by adding a state-dependent exploration (SDE) to current Deep Reinforcement Learning algorithms. 

In most RL algorithms the standard way of exploring the action space is to sample a noise vector from a Gaussian distribution which is then added to the agent predicted action, independently of the environment and the agent. SDE replaces the sampled noise with a state-dependent exploration function. This results in smoother exploration and less variance. Indeed, the Gaussian noise is added to the predicted action according to the state the action is taken on. This way, in a given episode, given the same state, the action is the same, reducing the variance with which the agent controls the robot. The standard deviation of the Gaussian noise is initialized randomly and then updated during training with the gradient of the policy, to be adapted to the task at hand.

Moreover, the state-dependent exploration idea is further improved by sampling the noise every n steps instead of every episode and by taking as input other features other than the state. Such changes address some of the issues of the state-dependent exploration (e.g. long episodes) and further increase the smoothness and performance of the exploration. SDE offers desired properties for the autonomous driving task we tackle in this thesis where a state-independent noise vector may damage the physical car due to the high-frequency of steering. Indeed, we activate SDE in the SAC algorithm when training the agent in our experiments.

\section{Learning to Drive - L2D}
% checked 

\citet{DBLP:journals/corr/abs-2008-00715} propose a framework to train a RL agent to control a physical self-driving car. In particular, they use a DonkeyCar, which we introduced in Section~\ref{sec:donkeycar}. To train the agent in the real world the authors adopt different training strategies, i.e. learning from pixels, using a state representation learning approach and using a model-based RL approach. They experimented with different physical tracks and their results show that learning directly from pixels is inefficient, ineffective, and, ultimately, does not lead to a policy that can drive the physical car successfully along the considered tracks. On the other hand, the state representation learning approach and the model-based RL approach show comparable performance.

In this thesis, we took inspiration from this work to train an RL agent in the real world. Similarly to \citet{DBLP:journals/corr/abs-2008-00715}, we follow the state representation learning approach by pre-training a Variational AE. Differently, we used a printed track that is larger than the tape-made tracks used by \citet{DBLP:journals/corr/abs-2008-00715} (i.e. 7 meters vs 11 meters measured by our track). Moreover, we experimented with different reset strategies and investigated the use of sim2real methods for transferring the policy from real to simulated and vice-versa.