@misc{robocarstore,
	title        = {{Robocar Store}},
	year         = 2022,
	key          = {robocarstore},
	howpublished = {\url{https://www.robocarstore.com/products/donkey-car-starter-kit-jetson-nano-edition}}
}

@article{stocco-mind,
	title        = {{Mind the Gap! A Study on the Transferability of Virtual vs Physical-world Testing of Autonomous Driving Systems}},
	author       = {Andrea Stocco and Brian Pulfer and Paolo Tonella},
	year         = 2022,
	journal      = {IEEE Transactions on Software Engineering},
	publisher    = {IEEE},
	volume       = {},
	url          = {https://arxiv.org/abs/2112.11255}
}

@misc{Unity,
	title        = {Unity3D.},
	year         = 2022,
	key          = {unity},
	howpublished = {\url{https://unity.com}}
}

@misc{learning-to-drive-in-5-minutes,
	author  = {Antonin Raffin},
	title   = {Learning to drive in 5 minutes},
	year    = {2020},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/araffin/learning-to-drive-in-5-minutes}},
}

@article{dqn,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={nature},
	volume={518},
	number={7540},
	pages={529--533},
	year={2015},
	publisher={Nature Publishing Group}
}


@misc{art:sac,
  doi = {10.48550/ARXIV.1801.01290},
  
  url = {https://arxiv.org/abs/1801.01290},
  
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{art:gan,
  doi = {10.48550/ARXIV.1406.2661},
  
  url = {https://arxiv.org/abs/1406.2661},
  
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}

@book{Alpaydin:2014:IML:2635955,
  added-at = {2015-03-29T21:33:02.000+0200},
  author = {Alpaydin, Ethem},
  biburl = {https://www.bibsonomy.org/bibtex/2697ffb3b7e245f062c40b55813b98038/clemensbaier},
  description = {Introduction to Machine Learning},
  interhash = {b8ed4b7ec46b3996c8d0191703408181},
  intrahash = {697ffb3b7e245f062c40b55813b98038},
  isbn = {0262028182, 9780262028189},
  keywords = {thesis},
  publisher = {The MIT Press},
  timestamp = {2015-03-29T21:33:02.000+0200},
  title = {Introduction to Machine Learning},
  year = 2014
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@InProceedings{Liang_2017_ICCV,
author = {Liang, Xiaodan and Hu, Zhiting and Zhang, Hao and Gan, Chuang and Xing, Eric P.},
title = {Recurrent Topic-Transition GAN for Visual Paragraph Generation},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{Isola_2017_CVPR,
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
title = {Image-To-Image Translation With Conditional Adversarial Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}


@article{karakan,
author = {Karacan, Levent and Akata, Zeynep and Erdem, Aykut and Erdem, Erkut},
year = {2016},
month = {12},
pages = {},
title = {Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts}
}

@misc{https://doi.org/10.48550/arxiv.1612.00835,
  doi = {10.48550/ARXIV.1612.00835},
  
  url = {https://arxiv.org/abs/1612.00835},
  
  author = {Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scribbler: Controlling Deep Image Synthesis with Sketch and Color},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.1710.10196,
  doi = {10.48550/ARXIV.1710.10196},
  
  url = {https://arxiv.org/abs/1710.10196},
  
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{pmlr-v70-zhang17b,
  title = 	 {Adversarial Feature Matching for Text Generation},
  author =       {Yizhe Zhang and Zhe Gan and Kai Fan and Zhi Chen and Ricardo Henao and Dinghan Shen and Lawrence Carin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {4006--4015},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/zhang17b/zhang17b.pdf},
  url = 	 {https://proceedings.mlr.press/v70/zhang17b.html},
  abstract = 	 {The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.}
}

@article{
doi:10.1126/science.1127647,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}

@INPROCEEDINGS{8456308,
  author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
  booktitle={2018 Picture Coding Symposium (PCS)}, 
  title={Deep Convolutional AutoEncoder-based Lossy Image Compression}, 
  year={2018},
  volume={},
  number={},
  pages={253-257},
  doi={10.1109/PCS.2018.8456308}}

@INPROCEEDINGS{7836672,  author={Gondara, Lovedeep},  booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},   title={Medical Image Denoising Using Convolutional Denoising Autoencoders},   year={2016},  volume={},  number={},  pages={241-246},  doi={10.1109/ICDMW.2016.0041}}

@INPROCEEDINGS{7926714,  author={Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},   title={Deep Feature Consistent Variational Autoencoder},   year={2017},  volume={},  number={},  pages={1133-1141},  doi={10.1109/WACV.2017.131}}

@INPROCEEDINGS{8852155,  author={Liu, Danyang and Liu, Gongshen},  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},   title={A Transformer-Based Variational Autoencoder for Sentence Generation},   year={2019},  volume={},  number={},  pages={1-7},  doi={10.1109/IJCNN.2019.8852155}}

@article{DBLP:journals/corr/abs-1802-04181,
  author    = {Timoth{\'{e}}e Lesort and
               Natalia D{\'{\i}}az Rodr{\'{\i}}guez and
               Jean{-}Fran{\c{c}}ois Goudou and
               David Filliat},
  title     = {State Representation Learning for Control: An Overview},
  journal   = {CoRR},
  volume    = {abs/1802.04181},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.04181},
  eprinttype = {arXiv},
  eprint    = {1802.04181},
  timestamp = {Mon, 13 Aug 2018 16:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-04181.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1910-01741,
  author    = {Denis Yarats and
               Amy Zhang and
               Ilya Kostrikov and
               Brandon Amos and
               Joelle Pineau and
               Rob Fergus},
  title     = {Improving Sample Efficiency in Model-Free Reinforcement Learning from
               Images},
  journal   = {CoRR},
  volume    = {abs/1910.01741},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01741},
  eprinttype = {arXiv},
  eprint    = {1910.01741},
  timestamp = {Sat, 23 Jan 2021 01:12:38 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01741.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v164-raffin22a,
  title = 	 {Smooth Exploration for Robotic Reinforcement Learning},
  author =       {Raffin, Antonin and Kober, Jens and Stulp, Freek},
  booktitle = 	 {Proceedings of the 5th Conference on Robot Learning},
  pages = 	 {1634--1644},
  year = 	 {2022},
  editor = 	 {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  volume = 	 {164},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08--11 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v164/raffin22a/raffin22a.pdf},
  url = 	 {https://proceedings.mlr.press/v164/raffin22a.html},
  abstract = 	 {Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL – often very successful in simulation – leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE enables a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance.}
}

@misc{https://doi.org/10.48550/arxiv.1901.08651,
  doi = {10.48550/ARXIV.1901.08651},
  
  url = {https://arxiv.org/abs/1901.08651},
  
  author = {Raffin, Antonin and Hill, Ashley and Traoré, René and Lesort, Timothée and Díaz-Rodríguez, Natalia and Filliat, David},
  
  keywords = {Machine Learning (cs.LG), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/abs-2008-00715,
  author    = {Ari Viitala and
               Rinu Boney and
               Juho Kannala},
  title     = {Learning to Drive Small Scale Cars from Scratch},
  journal   = {CoRR},
  volume    = {abs/2008.00715},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.00715},
  eprinttype = {arXiv},
  eprint    = {2008.00715},
  timestamp = {Fri, 07 Aug 2020 15:07:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-00715.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{vanOtterlo2012,
author="van Otterlo, Martijn
and Wiering, Marco",
title="Reinforcement Learning and Markov Decision Processes",
bookTitle="Reinforcement Learning: State-of-the-Art",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="3--42",
abstract="Situated in between supervised learning and unsupervised learning, the paradigm of reinforcement learning deals with learning in sequential decision making problems in which there is limited feedback. This text introduces the intuitions and concepts behind Markov decision processes and two classes of algorithms for computing optimal behaviors: reinforcement learning and dynamic programming. First the formal framework of Markov decision process is defined, accompanied by the definition of value functions and policies. The main part of this text deals with introducing foundational classes of algorithms for learning optimal behaviors, based on various definitions of optimality with respect to the goal of learning sequential decisions. Additionally, it surveys efficient extensions of the foundational algorithms, differing mainly in the way feedback given by the environment is used to speed up learning, and in the way they concentrate on relevant parts of the problem. For both model-based and model-free settings these efficient extensions have shown useful in scaling up to larger problems.",
isbn="978-3-642-27645-3",
doi="10.1007/978-3-642-27645-3_1",
url="https://doi.org/10.1007/978-3-642-27645-3_1"
}

