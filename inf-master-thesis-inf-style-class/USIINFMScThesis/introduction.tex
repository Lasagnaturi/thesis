\chapter{Introduction}

Reinforcement Learning (RL) is a branch of machine learning that has proven to be a very general framework to learn sequential decision-making tasks that are generally modeled as Markov Decision Processes (MDP) \citep{vanOtterlo2012} and without the need for labeled data. Reinforcement learning can operate in diverse situations as long as a clear reward can be applied. Under this framework, an RL agent interacts with an environment in discrete time steps and for each step, it observes the state of the environment and based on it, aims to take action that maximizes a given reward function. In recent years developments in RL have shown how it can deal with very high-complexity environments that can also change over time, first among all AlphaGo \citep{alphago} is an algorithm that is capable of playing the game of Go, notoriously the most challenging of classic AI games due to its huge search space and difficulty in evaluating board positions and moves. RL is capable of impressive performances when it is constrained in simulated environments, but uptake in real-world problems has been much slower given the even higher complexity and unpredictability of real-world environments. In particular, in robotics, training RL agents through trial and error has proven tedious and has shown several limitations, i.e. they can be very expensive due to the exploration phase and therefore the process should be data-efficient, actuators and sensors can introduce varying amounts of delay and noise, many robotic systems have a stricter form of constraint in their movements in comparison to simulated environments and the reward function can be very sparse due to the scarcity of sensors and information about the state. Interesting results in real-world applications comes often from very controlled environments, for example, solving a Rubik's cube requires a limited set of actions, and the reset of the cube to the initial state is very simple. The objective of this thesis is to learn an RL self-driving DonkeyCar, a scale remote-controlled electric car, first in simulation and then replicate the same result in a very uncontrolled real-world environment. The state-of-the-art self-driving cars algorithms, created by Google with Waymo and Tesla, rely on Supervised Learning (SL) techniques such as Convolutional Neural Networks (CNNs) for image processing, feature detection, and extraction, and Recurrent Neural Networks (RNNs) for processing temporal data. Even though supervised learning is very successful in autonomous driving, it does not come at no cost, it requires huge labeled datasets that need to be consistently updated to face new scenarios. Furthermore, SL methods typically are used to generate predictions about the surroundings of the car and upon that decisions are taken and do not take into account that each decision influences future events, which in turn influence future decisions. In other words, they try to imitate data but do not have the consciousness of the real world. RL, on the other hand, allows learning a policy, thereby creating models able to make their own decisions, take actions, react and adapt based on the feedback they receive. 

Since training an autonomous agent from raw images is expensive and has been proven unsuccessful \citep{DBLP:journals/corr/abs-2008-00715}, we first investigate a few Representation Learning (ReL) techniques such as AutoEncoders and Variational AutoEncoders. ReL is a technique designed to extract abstract features from data and reduce their complexity. Furthermore, techniques for a smooth exploration of the environment such as state-dependent exploration are implemented. A reward function that has proven to work in both simulated and real-world environments has been designed even if the sensors are scarce. In particular, as a first step of the experiments, we trained successfully a simulated proof-of-work RL agent that autonomously drives by taking actions based on what a camera sensor, with which it is equipped, sees as unique information about the surroundings. As a second step, the model is successfully replicated in a real-world environment with an investigation of the best training strategy in terms of the starting point of the car which crucially defines the learning success. 

Finally, a few unsuccessful experiments which need to be explored more are run in a Sim2Real procedure through the use of CycleGan \citep{CycleGAN2017}, where an agent trained in simulation is deployed in the real world and vice-versa, thus leading to cheaper training processes and more reliable benchmarking. 