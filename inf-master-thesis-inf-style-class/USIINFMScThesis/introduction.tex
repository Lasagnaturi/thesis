\chapter{Introduction}

Reinforcement Learning (RL) is a branch of machine learning that has proven to be a very general framework to learn sequential decision-making tasks modeled as Markov Decision Processes (MDP) \citep{vanOtterlo2012} without the need for labeled data. Under this framework, an RL agent interacts with an environment in discrete time steps, observing the state of the environment and deciding actions based on it, in order to maximize a given reward function to solve a certain task. Such task is usually \textit{episodic}, i.e., there are clear boundaries to determine when the task is achieved.

The first RL algorithms were tabular and could solve simple tasks and games~\cite{Sutton1998}. The advent of \textit{Deep Learning} made it possible for RL algorithms to scale towards interesting and challenging problems, where the state and/or the action space are too big to be stored in tables (e.g. images and continuous actions). Most notably, the first Deep RL (DRL) algorithm, called Deep Q Network, was able to surpass humans in most of the Atari games~\cite{atari}. Recent developments in RL have shown how the framework can deal with even more complex games, from the game of Go~\cite{alphago} to multiplayer games such as Starcraft II~\cite{alphastar} and Dota II~\cite{opeaifive}.

Despite the impressive performance, RL is still confined to simulated environments, where training data can be easily collected by running several instances in parallel and safety is not a primary concern. Indeed, progress towards bringing RL into the real world, has been much slower w.r.t. the progress made in simulation, given the higher complexity and unpredictability of real-world environments. One of the domains in which RL has been applied to the real world is robotics~\cite{smith2022walk,pmlr-v164-raffin22a,gu2017deep}. However, training RL agents to control physical robots in the real world presents several challenges that are absent in simulation. First of all, training cannot be easily parallelized and therefore the RL algorithm needs to be \textit{data-efficient}, i.e., it needs to be able to reuse the collected experience for training since acquiring data is costly. Secondly, the movements of the robot need to be taken into account both to protect the environment around the robot that might carry out unsafe actions during learning and to safeguard the robot itself that can be damaged by the high-frequency actions the robot is controlled with. Moreover, the number of sensors in the real world is limited compared to simulated environments. Having less sensors impacts the design of the reward function that is necessarily more \textit{sparse} (i.e. it gives less guidance to the agent during training) than in simulation. Finally, when the RL agent completes an episode, either successfully or unsuccessfully, a \textit{reset} of the environment is needed, in order to restore its initial state. Depending on the context, such task in the real world might require substantial human intervention.

Positive results in training RL agents to control robots in the real world often come from very constrained applications. For instance, the OpenAI team was able to train a RL agent to control a robotic hand in order to solve the Rubik's cube~\cite{rubik}. Other examples are robotic arms that have been trained to reach a certain object, pull or push a door and to pick an object and place it in a target location~\cite{gu2017deep}. The whole RL training process for such tasks can be almost entirely automated requiring little to no human supervision.

On the other hand, there are fewer tasks that have been addressed with RL in real-world uncontrolled environments~\cite{smith2022walk,DBLP:journals/corr/abs-2008-00715}. In this thesis we target with RL the problem of \textit{self-driving} and, in particular, we are interested in the \textit{lane-keeping} task in which the RL agent needs to control a car to drive along a predetermined track. This problem is usually tackled using the Supervised Learning (SL) approach both in academia and in industry. In such learning paradigm a dataset of driving experience is collected, usually composed of pairs of images and labels. In its simplest form the label is the recorded steering angle applied by the human driver corresponding to a certain image. Then, the dataset is used to train a Deep Neural Network (DNN) to minimize the error between the predicted steering angle and the ground truth label. Even though SL has proven successful for this task in the real world, it has some disadvantages. First, it requires huge labeled datasets that need to be constantly updated to take into account new scenarios. Furthermore, a SL model is trained offline and, as such, it cannot predict the effects that a certain action has on the environment. On the other hand, a RL agent is trained online, thereby being able to learn and adapt based on the effects of its actions, respecting the sequential nature of the task of driving.

%Interesting results in real-world applications comes often from very controlled environments, for example, solving a Rubik's cube requires a limited set of actions, and the reset of the cube to the initial state is very simple. The objective of this thesis is to learn an RL self-driving DonkeyCar, a scale remote-controlled electric car, first in simulation and then replicate the same result in a very uncontrolled real-world environment. The state-of-the-art self-driving cars algorithms, created by Google with Waymo and Tesla, rely on Supervised Learning (SL) techniques such as Convolutional Neural Networks (CNNs) for image processing, feature detection, and extraction, and Recurrent Neural Networks (RNNs) for processing temporal data. Even though supervised learning is very successful in autonomous driving, it does not come at no cost, it requires huge labeled datasets that need to be consistently updated to face new scenarios. Furthermore, SL methods typically are used to generate predictions about the surroundings of the car and upon that decisions are taken and do not take into account that each decision influences future events, which in turn influence future decisions. In other words, they try to imitate data but do not have the consciousness of the real world. RL, on the other hand, allows learning a policy, thereby creating models able to make their own decisions, take actions, react and adapt based on the feedback they receive. 

The objective of this thesis is to train a RL agent to control a physical 1:16 scale radio-controlled car (called DonkeyCar~\footnote{https://www.donkeycar.com/}) and make it drive along a physical track. The car is equipped with a camera, which is the only sensor the agent has to perceive its surroundings. We first addressed the problem in simulation, by using a high-fidelity simulator and a faithful representation of the physical track. We use simulation to carry out experiments to evaluate different training configurations in order to select the best choice when training in the real world. In particular, we experimented with different Representation Learning (ReL) techniques to speed up learning, as learning directly from raw images is known to be ineffective~\cite{DBLP:journals/corr/abs-2008-00715}. Moreover, we evaluated different reward functions and several reset strategies. Then, we transferred this knowledge to the real world and trained a RL agent which is able to learn to drive along the physical track in a reasonable amount of time.

%Since training an autonomous agent from raw images is expensive and has been proven unsuccessful \citep{DBLP:journals/corr/abs-2008-00715}, we first investigate a few Representation Learning (ReL) techniques such as AutoEncoders and Variational AutoEncoders. ReL is a technique designed to extract abstract features from data and reduce their complexity. Furthermore, techniques for a smooth exploration of the environment such as state-dependent exploration are implemented. A reward function that has proven to work in both simulated and real-world environments has been designed even if the sensors are scarce. In particular, as a first step of the experiments, we trained successfully a simulated proof-of-work RL agent that autonomously drives by taking actions based on what a camera sensor, with which it is equipped, sees as unique information about the surroundings. As a second step, the model is successfully replicated in a real-world environment with an investigation of the best training strategy in terms of the starting point of the car which crucially defines the learning success. 

Finally, we also experimented with \textit{sim2real} (simulation to real) techniques that can be used to transfer knowledge from one environment (e.g. in simulation) to another one (e.g. in the real world). Our attempts can be useful to understand the direction of future endeavors at addressing the same problem.

%Finally, a few unsuccessful experiments which need to be explored more are run in a Sim2Real procedure through the use of CycleGan \citep{CycleGAN2017}, where an agent trained in simulation is deployed in the real world and vice-versa, thus leading to cheaper training processes and more reliable benchmarking. 