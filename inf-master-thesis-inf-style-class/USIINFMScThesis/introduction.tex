\chapter{Introduction}

Reinforcement Learning has proven to be a very general framework to learn decision making task that are generally modelled as Markov Decision Processes (MDP), so, as a discrete-time stochastic control process \citep{vanOtterlo2012}. Even though RL is well established and has been widely investigated in simulated environments where an agent needs to select the best set of actions to accomplish a certain task, moving it the real world is often tedius. The focus of this thesis is on DonkeyCar, a cheap remote controlled car, and in a reinforcement learning algorithm so that it can learn to drive autonomously on a toy track through a camera. Firstly, the feasability of this goal is investigated in a simulated environment, and then moved to the real world. However, as described by \citet{DBLP:journals/corr/abs-2008-00715}, it is well know that learning this task, in this setting, is not possible from raw images. Thus, representation learning is used to compress observations and extract relevant features. Alongside, an investigation is done to determine wheter in our context is better to use an autoencoder or a variational autoencoder. Furthermore, given that in real world we lack the supervision of the environment, several attempt are made to define a reward function that is suitable in both the simulated and the real environment. In fact, the only supervision available in this setting, is a human that can tell the algoritmh when the car is off track and the episode must terminate, in simulation, instead, a measure of the cross track error is available.  Established that the model can learn, we reproduce successfully the simulated agent in real world. Finally, an unsucceful attempt is made in adapting the already trained simulated agent to the real world, without extra training, with the use of a cyclegan that is capable of transforming a simulated image into a pseudo-real image. In pratice, we want to make the agent see images similar to those used in training. In essence, we wanted to propose an extremly simple sim to real framework, which would drastically reduce training costs and would make simpler and more reliable the benchmarking. Given that the DonkeyCar's microcontroller is not powerful enough, an off-policy reinforcement algorithm (SAC) was choosen in order reduce the workload of the microcontroller and move the actual training to an external server with enough resources to accomplish the desired goals.